{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://github.com/hamelsmu/Seq2Seq_Tutorial/blob/master/notebooks/Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuptservol/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)\n",
    "from ktext.preprocess import processor\n",
    "import dill as dpickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='data/github-issues/'\n",
    "MODEL_PATH=f'{PATH}model/'\n",
    "GITHUB_ISSUES = f'{PATH}github_issues.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--  1 kuptservol kuptservol 2.7G Sep 19 14:21 github_issues.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah {PATH} | grep github_issues.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 900,000 rows 3 columns\n",
      "Test: 100,000 rows 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3548140</th>\n",
       "      <td>\"https://github.com/BBasile/Coedit/issues/185\"</td>\n",
       "      <td>diff dialog shown twice on external modification</td>\n",
       "      <td>the diff dialog causes a loss of focus which leads to a double check.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577963</th>\n",
       "      <td>\"https://github.com/samsung-cnct/k2/issues/326\"</td>\n",
       "      <td>fix 'clean up releases' to avoid needing to ignore the failure</td>\n",
       "      <td>if we can ignore this failure, we should be able to test and see that the work doesn't actually need to be done and just not execute this. seeing failures that are ignored irritates me. task roles/kraken.services : clean up releases failed: localhost item={u'name': u'kubedns', u'namespace': u'kube-system', u'chart': u'kubedns', u'repo': u'atlas', u'version': u'0.1.0', u'values': {u'cluster_ip': u'10.32.0.2', u'dns_domain': u'cluster.local'}} =&gt; { changed : true, cmd : helm , delete , --purge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19327</th>\n",
       "      <td>\"https://github.com/oyyd/cheerio-without-node-native/issues/5\"</td>\n",
       "      <td>a very nice module</td>\n",
       "      <td>thany you! i am looking for a module which will be used in react-native it suits me fine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              issue_url  \\\n",
       "3548140                  \"https://github.com/BBasile/Coedit/issues/185\"   \n",
       "2577963                 \"https://github.com/samsung-cnct/k2/issues/326\"   \n",
       "19327    \"https://github.com/oyyd/cheerio-without-node-native/issues/5\"   \n",
       "\n",
       "                                                            issue_title  \\\n",
       "3548140                diff dialog shown twice on external modification   \n",
       "2577963  fix 'clean up releases' to avoid needing to ignore the failure   \n",
       "19327                                                a very nice module   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        body  \n",
       "3548140                                                                                                                                                                                                                                                                                                                                                                                                                                                the diff dialog causes a loss of focus which leads to a double check.  \n",
       "2577963  if we can ignore this failure, we should be able to test and see that the work doesn't actually need to be done and just not execute this. seeing failures that are ignored irritates me. task roles/kraken.services : clean up releases failed: localhost item={u'name': u'kubedns', u'namespace': u'kube-system', u'chart': u'kubedns', u'repo': u'atlas', u'version': u'0.1.0', u'values': {u'cluster_ip': u'10.32.0.2', u'dns_domain': u'cluster.local'}} => { changed : true, cmd : helm , delete , --purge...  \n",
       "19327                                                                                                                                                                                                                                                                                                                                                                                                                               thany you! i am looking for a module which will be used in react-native it suits me fine  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data sample 2M rows (for speed of tutorial)\n",
    "traindf, testdf = train_test_split(pd.read_csv(GITHUB_ISSUES)\n",
    "#                                    .sample(n=2000000)\n",
    "                                   .sample(n=1000000)\n",
    "                                   , test_size=.10)\n",
    "\n",
    "\n",
    "#print out stats about shape of data\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "\n",
    "# preview data\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the diff dialog causes a loss of focus which leads to a double check.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_body_raw = traindf.body.tolist()\n",
    "train_title_raw = traindf.issue_title.tolist()\n",
    "#preview output of first element\n",
    "train_body_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 189 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 39 sec\n",
      "WARNING:root:Finished parsing 900,000 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 28 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 6.13 s, total: 1min 54s\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
    "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " the diff dialog causes a loss of focus which leads to a double check. \n",
      "\n",
      "after pre-processing:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    3 1406 1152  965    5 2041   11 1267   63 2074    4    5  727  150] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
    "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 33 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 6 sec\n",
      "WARNING:root:Finished parsing 900,000 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 9 sec\n"
     ]
    }
   ],
   "source": [
    "title_pp = processor(append_indicators=True, keep_n=4500, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " diff dialog shown twice on external modification\n",
      "after pre-processing:\n",
      " [   2 1608  594  704  956   10  576 2603    3    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_title_raw[0])\n",
    "print('after pre-processing:\\n', train_title_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open(f'{MODEL_PATH}body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open(f'{MODEL_PATH}title_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save(f'{MODEL_PATH}train_title_vecs.npy', train_title_vecs)\n",
    "np.save(f'{MODEL_PATH}train_body_vecs.npy', train_body_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (900000, 70)\n",
      "Shape of decoder input: (900000, 11)\n",
      "Shape of decoder target: (900000, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs(f'{MODEL_PATH}train_body_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(f'{MODEL_PATH}train_title_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for data/github-issues/model/body_pp.dpkl: 8,002\n",
      "Size of vocabulary for data/github-issues/model/title_pp.dpkl: 4,502\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor(f'{MODEL_PATH}body_pp.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor(f'{MODEL_PATH}title_pp.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 300\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 300)    1350600     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 300)          2942700     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 300),  540900      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 4502)   1355102     Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 6,191,702\n",
      "Trainable params: 6,189,902\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-99ecfa815c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseq2seq_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mviz_model_architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mseq2seq_Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mviz_model_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2seq_Model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai/courses/dl1/seq2seq_utils.py\u001b[0m in \u001b[0;36mviz_model_architecture\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mviz_model_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m\"\"\"Visualize model architecture in Jupyter notebook.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792000 samples, validate on 108000 samples\n",
      "Epoch 1/7\n",
      "792000/792000 [==============================] - 271s 342us/step - loss: 2.9630 - val_loss: 2.5559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuptservol/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7\n",
      "792000/792000 [==============================] - 263s 333us/step - loss: 2.4274 - val_loss: 2.4364\n",
      "Epoch 3/7\n",
      "792000/792000 [==============================] - 265s 334us/step - loss: 2.2219 - val_loss: 2.4035\n",
      "Epoch 5/7\n",
      "792000/792000 [==============================] - 265s 335us/step - loss: 2.1657 - val_loss: 2.4074\n",
      "Epoch 6/7\n",
      "792000/792000 [==============================] - 265s 334us/step - loss: 2.1205 - val_loss: 2.4153\n",
      "Epoch 7/7\n",
      "792000/792000 [==============================] - 265s 334us/step - loss: 2.0824 - val_loss: 2.4274\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'tutorial_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 7\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuptservol/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "seq2seq_Model.save(f'{MODEL_PATH}seq2seq_model_tutorial.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 47568 =================\n",
      "\n",
      "\"https://github.com/francineloza/HIM_Operations/issues/58\"\n",
      "Issue Body:\n",
      " hi francine, i pushed a login to the server anm: angrejoaagrajo5253, password: 9456 , but got a call from our field manager that there was a mistake in the form. we needed to change the login to angrejo5253 password: 9456 , so i pushed more data to the server with that. basically, we have two anms in our system now: anrejoaagrajo5253 and anrejo5253. i'm not sure what we do about the first anm but just wanted to flag this so we don't end up sending a report to the govt saying this anm isn't using her tablet. thanks, and i'm really sorry for the inconvenience : \n",
      "\n",
      "Original Title:\n",
      " delete anm angrejoaagrajo5253\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " login form is not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 23975 =================\n",
      "\n",
      "\"https://github.com/PatchworkBoy/homebridge-edomoticz/issues/100\"\n",
      "Issue Body:\n",
      " hi, i had to change my z-wave stick and thus, exclude then include all my devices. my fibaro wallplugs fgwpe used to appear in homekit before my resintall. but since then, all of them appear and work in domoticz, appear in homebridge logs edomoticz initializing platform accessory 'prise grenier'... , but do not show up in homekit. any idea what i'm doing wrong? note that all my other z-wave devices appear correctly in homekit. thanks. \n",
      "\n",
      "Original Title:\n",
      " fibaro fgwpe-102 not appearing in homekit\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " not seeing all devices\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 22786 =================\n",
      "\n",
      "\"https://github.com/StrangeLoopGames/EcoIssues/issues/1811\"\n",
      "Issue Body:\n",
      " the patch i put it on didnt turn brown to represent the stockpile. \n",
      "\n",
      "Original Title:\n",
      " stockpiles dont immediately appear on the minimap\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " patch number 2\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 52006 =================\n",
      "\n",
      "\"https://github.com/tidyverse/tidyr/issues/367\"\n",
      "Issue Body:\n",
      " this is especially confusing for beginners: why do we need to use quotes when calling separate , but not for unite or spread or gather ? r suppresspackagestartupmessages library tidyverse table3 %>% separate rate, into = c cases , population > a tibble: 6 x 4 > country year cases population > <chr> <int> <chr> <chr> > 1 afghanistan 1999 745 19987071 > 2 afghanistan 2000 2666 20595360 > 3 brazil 1999 37737 172006362 > 4 brazil 2000 80488 174504898 > 5 china 1999 212258 1272915272 > 6 china 2000 213766 1280428583 it would be more natural to be able to write just r table3 %>% separate rate, cases, population cc @mehranhosseini. \n",
      "\n",
      "Original Title:\n",
      " inconsistent interface for separate into = c ...\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " separate quote for multiple values\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 76587 =================\n",
      "\n",
      "\"https://github.com/github/markup/issues/1022\"\n",
      "Issue Body:\n",
      " see the 2 first lines: https://raw.githubusercontent.com/snaipe/criterion/bleeding/readme.md very recently, this rendered properly as a header with only an image. now, it breaks, and renders the ========= verbatim below the image. \n",
      "\n",
      "Original Title:\n",
      " raw html headers not rendering as before\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " header image is not rendered correctly\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 62088 =================\n",
      "\n",
      "\"https://github.com/koorellasuresh/UKRegionTest/issues/32561\"\n",
      "Issue Body:\n",
      " first from flow in uk south \n",
      "\n",
      "Original Title:\n",
      " first from flow in uk south\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 81688 =================\n",
      "\n",
      "\"https://github.com/yahoo/fili/issues/282\"\n",
      "Issue Body:\n",
      " if there are no slices with intervals for any column metric or dimension , the health check should fail because there's no data query we can successfully respond to. as long as there is at least one interval in one column on one slice, we can, in theory, respond to at least one query. \n",
      "\n",
      "Original Title:\n",
      " add a health check that fails if there are no intervals available for any slices\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " check if health check fails\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 76501 =================\n",
      "\n",
      "\"https://github.com/excelwebzone/EWZRecaptchaBundle/issues/189\"\n",
      "Issue Body:\n",
      " it is working for my registration form. i put the same twig code in login twig file. but it allow sign-in without validation recaptcha click. help me please. i have deadline for project. \n",
      "\n",
      "Original Title:\n",
      " how to use for login form. not validate recaptcha\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " registration form not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 36590 =================\n",
      "\n",
      "\"https://github.com/ethereumproject/mist/issues/46\"\n",
      "Issue Body:\n",
      " hi, my wallet can not sycronize it freeze while downloading blocks. block 3,099,823 of 3,420,114 and waiting for hours. what it could be? thanks system information version: 0.0.0 os & version: windows/linux/osx node type: eth/getc default please check the already existing issues to keep duplicates at a minimum. furthermore several work-arounds have been collected in the mist-troubleshooting-guide https://github.com/ethereumproject/wiki/wiki/mist-troubleshooting-guide . if possible add the following to your report: - screenshots - check the console, of mist ctrl/cmd + alt + i and take a screenshot - log files go to menu -> accounts -> backup -> application data and upload as zip-archive : - osx: ~/library/application support/mist/node.log - windows: %appdata%/roaming/mist/node.log - linux: ~/.config/mist/node.log \n",
      "\n",
      "Original Title:\n",
      " can not sycronized\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " can not sync wallet\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 38340 =================\n",
      "\n",
      "\"https://github.com/girder/monkeybrains/issues/23\"\n",
      "Issue Body:\n",
      " the footprint of this plugin is much bigger than it needs to be because we are pulling in the entire d3 package. if we move to the latest major version of d3, it's been modularized and we can just pull in the few bits we need. \n",
      "\n",
      "Original Title:\n",
      " port d3 dependency to modern version\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " reduce the size of the plugin\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 2492 =================\n",
      "\n",
      "\"https://github.com/MatisiekPL/Czekolada/issues/4268\"\n",
      "Issue Body:\n",
      " ┆attachments: <a href= https:& x2f;& x2f;github.com& x2f;matisiekpl& x2f;czekolada& x2f;issues& x2f;4265 >https:& x2f;& x2f;github.com& x2f;matisiekpl& x2f;czekolada& x2f;issues& x2f;4265</a> \n",
      "\n",
      "Original Title:\n",
      " issue 4265: issue 4264: issue 4261: issue 4260: issue 4257: issue 4256: issue 4253: issue 4252: issue 4249: issue 4248: issue 4245: issue 4244: issue 4242: issue 4239: issue 4238: issue 4235: issue 4234: issue 4231: issue 4230: issue 4227: issue 4226: issu\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " issue number issue number issue number issue number issue number issue number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 77828 =================\n",
      "\n",
      "\"https://github.com/mattgodbolt/Miracle/issues/6\"\n",
      "Issue Body:\n",
      " how can we load a rom from url? for example, if we have a compiled homebrew rom host somewhere in webhost services like altervista, how can we add its url in the whole emulator url, just like we do from emulators like webmsx? like http://webmsx.org/?rom=http://nitrofurano.altervista.org/retrocoding/msx/roms/cmjn.rom \n",
      "\n",
      "Original Title:\n",
      " loading rom from url\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " load a url with a usb host\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 84199 =================\n",
      "\n",
      "\"https://github.com/ElemeFE/element/issues/4325\"\n",
      "Issue Body:\n",
      " <!-- generated by https://eleme-issue.surge.sh do not remove --> existing component 是 component name el-table description 实际的表格中可能会存在 show-overflow-tooltip 的内容很长，导致显示的时候，宽度是整屏的情况。所以希望能设置tooltip的最大宽度，可以是整个表格设置或者单列设置。另外就是我这种情况有什么好的解决方案吗？ eg：http://jsfiddle.net/rmpj2zhf/1/ <!-- generated by https://eleme-issue.surge.sh do not remove --> \n",
      "\n",
      "Original Title:\n",
      " feature request 表格的 shot-overflow-tooltip 是否可以设置最大宽度\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " feature request el table component\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 95697 =================\n",
      "\n",
      "\"https://github.com/FluxionNetwork/fluxion/issues/128\"\n",
      "Issue Body:\n",
      " ! screenshot https://user-images.githubusercontent.com/30051406/29865959-f23a19f4-8d44-11e7-87ff-6d01850e788e.png what is the current behaviour? final step: 4 screens shows up, and hangs. no reconfirm password. from the connected devices no disconnections from the real ap obviously no redirect to login page. note : all steps working fine with no errors, till the 4 screens specs real ap -> netgear. running fluxion by -> export fluxionwikillprocesses=1; ./fluxion.sh testing with connected devices -> 3 android phones paste the output of ./script/diagnostics.sh : root@kali:~/fluxion ./scripts/diagnostics.sh wlan0 fluxion info fluxion v3.2 bash info gnu bash, version 4.4.12 1 -release x86_64-pc-linux-gnu copyright c 2016 free software foundation, inc. license gplv3+: gnu gpl version 3 or later <http://gnu.org/licenses/gpl.html> this is free software; you are free to change and redistribute it. there is no warranty, to the extent permitted by law. path: /bin/bash interface wlan0 info device: phy2 driver: 8812au chipset: realtek semiconductor corp. rtl8812au 802.11a/b/g/n/ac wlan adapter injection test: injection is working! xterm info version: xterm 330 path: /usr/bin/xterm test: xserver/xterm success! hostapd info hostapd v2.4 user space daemon for ieee 802.11 ap management, ieee 802.1x/wpa/wpa2/eap/radius authenticator copyright c 2002-2015, jouni malinen <j@w1.fi> and contributors path: /usr/sbin/hostapd aircrack-ng info aircrack-ng 1.2 rc4 - c 2006-2015 thomas d'otreppe http://www.aircrack-ng.org system info linux version 4.9.0-kali4-amd64 devel@kali.org gcc version 6.3.0 20170516 debian 6.3.0-18 1 smp debian 4.9.30-2kali1 2017-06-22 \n",
      "\n",
      "Original Title:\n",
      " captive portal attack in progress - no actions\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " number 2 client hangs on lan\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 13844 =================\n",
      "\n",
      "\"https://github.com/joel-costigliola/assertj-core/issues/1069\"\n",
      "Issue Body:\n",
      " summary as far as i know, null is considered as blank as blank https://commons.apache.org/proper/commons-lang/apidocs/org/apache/commons/lang3/stringutils.html isblank-java.lang.charsequence- is superset of null, empty string or blank characters only example this passes: string s = null; assertthat s .isnotblank ; java 8 specific ? no : create pull request from the 2.x branch \n",
      "\n",
      "Original Title:\n",
      " null is not blank?!\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " null value in string\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 26752 =================\n",
      "\n",
      "\"https://github.com/telerik/nativescript-ui-feedback/issues/366\"\n",
      "Issue Body:\n",
      " tell us about the problem weeknumbers are not shown in dayview which platform s does your issue occur on? ios please provide the following version numbers that your issue occurs with: - progress nativescript ui version: next please tell us how to recreate the issue in as much detail as possible. 1. start sdkangular -> calendar -> cellstyling 2. from options menu select day is there code involved? if so, please share the minimal amount of code needed to recreate the problem. code from the example html <rc:radcalendar.dayviewstyle> <rc:calendardayviewstyle backgroundcolor= dd855c showtitle= true showweeknumbers= true showdaynames= true > <rc:calendardayviewstyle.todaycellstyle> <rc:daycellstyle cellbackgroundcolor= dd855c cellborderwidth= 1 cellbordercolor= f1e8ca celltextcolor= 745151 celltextfontname= times new roman celltextfontstyle= bold celltextsize= 14 /> </rc:calendardayviewstyle.todaycellstyle> .. \n",
      "\n",
      "Original Title:\n",
      " calendar ios in dayview showweeknumbers= true doesn't apply\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " how to change the calendar day from the day\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 87384 =================\n",
      "\n",
      "\"https://github.com/pycontribs/django-alexa/issues/26\"\n",
      "Issue Body:\n",
      " thanks for putting together this library, it looks like it will be a real time saver! i'm a noob with alexa, but it seems like the example is not implementing the built-in intents properly. this could be a complete oversight on my part of course, but i thought the built-in intents were supposed to be defined as amazon.helpintent , for example, in the intent schema. however, in the the example you have, you define the helpintent as helpintent . is this intentional? i thought it might be because of the alexa.py file function names would be awkward otherwise. however, this seems to then override the built-in intent utterances that amazon provides. thanks for your help! \n",
      "\n",
      "Original Title:\n",
      " question regarding built-in intents\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " example not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 24866 =================\n",
      "\n",
      "\"https://github.com/pluginsGLPI/ocsinventoryng/issues/92\"\n",
      "Issue Body:\n",
      " hi, the automatic synchronization is not working, it's working only the manual import. i've got the following set up: glpi 9.1.2 plugin 1.3.3 synchronization method: expert fully authomatic, for large configuration on automatic action ocsng - launch ocsng synchronization script is scheduled and its' running fine. any idea? what should i check? thanks \n",
      "\n",
      "Original Title:\n",
      " automatic synchronization not working\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " automatic synchronization of number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 58440 =================\n",
      "\n",
      "\"https://github.com/bloomberg/bqplot/issues/498\"\n",
      "Issue Body:\n",
      " just a question: i'm using the new graph mark. python tt = tooltip fields=node_attrs_list graph = graph node_data=node_data, link_data=link_data, link_type='line', colors=color_array, tooltip=tt, directed=false instead of adding the same tooltip to all nodes in the graph, i would like to display different tooltip for different nodes ie, some nodes might have a 'size' attribute, others might not. is there any way to do this? or if not, is there a way to get the tooltip to not display empty attributes? \n",
      "\n",
      "Original Title:\n",
      " customize tooltip for different elements of a mark?\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " different tooltip for nodes\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 92866 =================\n",
      "\n",
      "\"https://github.com/libcg/bfp/issues/4\"\n",
      "Issue Body:\n",
      " once we get a base implementation we should be able to wire muparser to bfp to get an interactive shell. \n",
      "\n",
      "Original Title:\n",
      " add muparser example\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " interactive shell for interactive shell\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 69448 =================\n",
      "\n",
      "\"https://github.com/jonaprieto/athena/issues/23\"\n",
      "Issue Body:\n",
      " consider the following example from the test folder. $ cat test/prop-pack/problems/prop-metis/prop-13.tptp ... fof 'prop_13', conjecture, p | q & r <=> p | q & p | r . fof subgoal_0, plain, p | q & r & ~ p => q , inference strip, , 'prop_13' . ... the problem comes with 'prop_13' . $ athena prop-13.tstp athena: maybe.fromjust: nothing \n",
      "\n",
      "Original Title:\n",
      " parsing tstp when identifiers contains underscore and are quoted\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " props not found on the same instance\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 18066 =================\n",
      "\n",
      "\"https://github.com/xproc/3.0-specification/issues/193\"\n",
      "Issue Body:\n",
      " the example @ndw provided in his mail http://markmail.org/search/?q=list%3axproc-dev query:list%3axproc-dev+page:2+mid:exbrvizz3bbplrjb+state:results to me raises another point with @select on p:with-input etc.: suppose i have a step which accepts say text/plain and i have an xml document containing a text node as child of a section element. can i use <p:with-input select= //section/text > with the xml document as default readable port or context node if you prefer ? the problem is at which point we will say, that the document to arrive on the text step does not match the expected content type text/plain . if we do it before the evaluation @select an error has to be raised because there its application/xml . if we consider the result of @select as we do to assess whether its a sequence or not , no error should be raised. i think you can make up the same kind of example for any xdm defined type as value of @select. i hope, i made my point clear. if not, please tell me so and i try to provide an elaborated example. \n",
      "\n",
      "Original Title:\n",
      " @select and document-properties\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " input fields not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 92130 =================\n",
      "\n",
      "\"https://github.com/wangkanai/Detection/issues/27\"\n",
      "Issue Body:\n",
      " i am opening this issue for convenience. \n",
      "\n",
      "Original Title:\n",
      " add detectionservice.cs tests and refactoring.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " is this a demo\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 26177 =================\n",
      "\n",
      "\"https://github.com/openai/rllab/issues/138\"\n",
      "Issue Body:\n",
      " hi, out of curiosity -- and for implementing my own stuff -- i was wondering why rllab has its own implementation of gru, and doesn't use the one in lasagne? is that because the lasagne stateful implementation i.e. ability to compute output per time step, remembering previous hidden state isn't working / satisfactory? \n",
      "\n",
      "Original Title:\n",
      " custom gru implementation\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " why is the implementation of the implementation of the implementation of the\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 85412 =================\n",
      "\n",
      "\"https://github.com/ionic-team/ionic/issues/12516\"\n",
      "Issue Body:\n",
      " ionic version: 3.6.0 how to fix this error that says cannot read property match ! ion_error https://user-images.githubusercontent.com/11710306/28755732-bf7fcee6-7561-11e7-9a87-35c7126f7e25.jpg \n",
      "\n",
      "Original Title:\n",
      " cannot read property 'match' of undefined typeerror\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " ionic number 2 not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 96163 =================\n",
      "\n",
      "\"https://github.com/funcool/buddy-sign/issues/47\"\n",
      "Issue Body:\n",
      " from the perspective of the caller of validating the incoming jwt, they allow themselves, a single audience, of possibly many audiences listed in the claim. conversely, they also may allow tokens generated by many issuers, one of which is presented in the claim. so, the validation logic should a single audience to match against a list of audiences in the token , and a list of issuers to match against the single issuer in the token . entails changing https://github.com/funcool/buddy-sign/blob/master/src/buddy/sign/jwt.clj l26 to when and iss let iss-claim :iss claims if coll? iss not-any? iss-claim {iss} not= iss-claim iss throw ex-info str issuer does not match iss {:type :validation :cause :iss} to something that is similar to the aud validation, but with inverse item-to-list matching. \n",
      "\n",
      "Original Title:\n",
      " iss validation should be against a collection whitelist\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add validation of the token\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 36341 =================\n",
      "\n",
      "\"https://github.com/unfoldingWord-dev/translationCore/issues/2242\"\n",
      "Issue Body:\n",
      " 1. download and install private beta v0.7.0. 2. use tc developer mode. 3. open all projects and tools to ensure they open and will open faster subsequently. \n",
      "\n",
      "Original Title:\n",
      " 2 help perry use tc with gospels\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " update to use the latest version of the project\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 30536 =================\n",
      "\n",
      "\"https://github.com/quidsup/notrack/issues/225\"\n",
      "Issue Body:\n",
      " found couple of domains that render some leading websites broken: purch.com -- no images on www.tomshardware.com adobedtm.com -- no videos on www.redbull.tv \n",
      "\n",
      "Original Title:\n",
      " request to remove from blacklist\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " broken image domains\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 87401 =================\n",
      "\n",
      "\"https://github.com/DSpeichert/cloudvcl/issues/79\"\n",
      "Issue Body:\n",
      " ! ip owner history https://cloud.githubusercontent.com/assets/6411490/25789883/68021654-3382-11e7-9e69-a3053f7e1217.png ip owner historys should be renamed to either ip owner history or ip owner histories. \n",
      "\n",
      "Original Title:\n",
      " ip owner history\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " ip owner history\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 40509 =================\n",
      "\n",
      "\"https://github.com/Microsoft/vscode/issues/20586\"\n",
      "Issue Body:\n",
      " - vscode version: code 1.9.0 27240e71ef390bf2d66307e677c2a333cebf75af, 2017-02-02t08:31:00.827z - os version: windows_nt ia32 10.0.14393 - extensions: |extension|author|version| |---|---|---| |jslint|ajhyndman|1.2.1| |jshint|dbaeumer|0.10.13| |csharp|ms-vscode|1.1.6| --- steps to reproduce: 1. 2. \n",
      "\n",
      "Original Title:\n",
      " vs code crash\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " i can t see the code in the vscode\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 29357 =================\n",
      "\n",
      "\"https://github.com/uwcart/mapstudy/issues/53\"\n",
      "Issue Body:\n",
      " sometimes when zooming in and out of a map i get the console error below and the map gets 'stuck'. in other words, it will no longer re-render the map on zoom, leaving it half-rendered on pan and zoom, as seen below. firefox 54.0, osx 10.11.13 i don't know how to reproduce this error, it just happens periodically when using a map. occurs in different symbolization types as well. ! screen shot 2017-06-28 at 12 25 27 pm https://user-images.githubusercontent.com/8650453/27648725-6fc0e1ec-5bfd-11e7-8f61-00c0452a9e1b.png ! screen shot 2017-06-28 at 12 25 35 pm https://user-images.githubusercontent.com/8650453/27648728-71836c16-5bfd-11e7-95b4-9d93f4e55cd6.png \n",
      "\n",
      "Original Title:\n",
      " bug: render error\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " map sometimes stuck in a loop\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 92617 =================\n",
      "\n",
      "\"https://github.com/andybotting/xbmc-addon-afl-video/issues/1697\"\n",
      "Issue Body:\n",
      " automatic bug report from end-user. environment add-on name: afl video add-on id: plugin.video.afl-video add-on version: 1.7.7 kodi version: 17.1-rc1 git:20170223-1a80820 python version: 2.7.11 default, jan 16 2016, 16:21:17 msc v.1900 32 bit intel operating system: win32 ip address: 58.96.37.2 2.37.96.58.static.exetel.com.au isp: unknown lookup failure kodi url: ?category=live%20matches python path: c:\\users\\daniel\\appdata\\roaming\\kodi\\addons\\plugin.video.afl-video\n",
      "c:\\users\\daniel\\appdata\\roaming\\kodi\\addons\\script.module.beautifulsoup4\\lib\n",
      "c:\\users\\daniel\\appdata\\roaming\\kodi\\addons\\script.module.requests\\lib\n",
      "c:\\program files x86 \\kodi\\system\\python\\dlls\n",
      "c:\\program files x86 \\kodi\\system\\python\\lib\n",
      "c:\\program files x86 \\kodi\\python27.zip\n",
      "c:\\program files x86 \\kodi\\system\\python\\lib\\plat-win\n",
      "c:\\program files x86 \\kodi\\system\\python\\lib\\lib-tk\n",
      "c:\\program files x86 \\kodi\n",
      "c:\\program files x86 \\kodi\\system\\python\n",
      "c:\\program files x86 \\kodi\\system\\python\\lib\\site-packages\n",
      "c:\\users\\daniel\\appdata\\roaming\\kodi\\addons\\plugin.video.afl-video\\resources\\lib traceback traceback most recent call last : file c:\\users\\daniel\\appdata\\roaming\\kodi\\addons\\plugin.video.afl-video\\resources\\lib\\videos.py , line 34, in make_list videos = comm.get_videos category file c:\\users\\daniel\\appdata\\roaming\\kodi\\addons\\plugin.video.afl-video\\resources\\lib\\comm.py , line 197, in get_videos upcoming_videos = get_round 'latest', true file c:\\users\\daniel\\appdata\\roaming\\kodi\\addons\\plugin.video.afl-video\\resources\\lib\\comm.py , line 229, in get_round matches = rnd.find 'matches' .getchildren attributeerror: 'nonetype' object has no attribute 'getchildren' full log https://gist.github.com/e342ac58d1bc68a369fafd6b9eff9b57 \n",
      "\n",
      "Original Title:\n",
      " xbmcbot - attributeerror: 'nonetype' object has no attribute 'getchildren'\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " indexerror list index out of range\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 78776 =================\n",
      "\n",
      "\"https://github.com/mikeybkats/MoMM2/issues/10\"\n",
      "Issue Body:\n",
      " possible to remove browsing category ? ! screen shot 2017-08-11 at 3 37 33 pm https://user-images.githubusercontent.com/30940080/29234330-0fe83682-7eab-11e7-9e59-f23792c8d8df.png http://staging.methodsofamodernmale.com/?cat=7 url \n",
      "\n",
      "Original Title:\n",
      " sub header - category page\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " remove category buttons\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 85895 =================\n",
      "\n",
      "\"https://github.com/stereolabs/zed-python/issues/13\"\n",
      "Issue Body:\n",
      " to convert an sl::mat to a cv::mat, we provide an slmat2cvmat function in the opencv . so is there a slmat2cvmat function that could use? \n",
      "\n",
      "Original Title:\n",
      " is there a slmat2cvmat function in zed-python?\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " mat function for mat cv mat cv mat\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 32418 =================\n",
      "\n",
      "\"https://github.com/actframework/actframework/issues/217\"\n",
      "Issue Body:\n",
      " some dbservice , e.g. ebeanservice might have trouble https://github.com/ebean-orm/ebean/issues/620 while initialization after app refreshed in dev mode . in order to workaround that issue, we need to support asynchronous dbservice initialization process \n",
      "\n",
      "Original Title:\n",
      " support initialize dbservice asynchronously\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " initialization process initialization\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 60114 =================\n",
      "\n",
      "\"https://github.com/CarragherLab/cptools2/issues/18\"\n",
      "Issue Body:\n",
      " sometimes fails silently despite detecting the plates, returning empty command files. should check that the commands files are not empty as a final sanity check. \n",
      "\n",
      "Original Title:\n",
      " check commands files are not empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " check command silently ignores empty files\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 54124 =================\n",
      "\n",
      "\"https://github.com/electron/electron.atom.io/issues/663\"\n",
      "Issue Body:\n",
      " i would like to know how to identify the new applications published on the page, since previously the new applications simpr were at the end, and there was no such order as the alphabet. i think the new organizational structure and the filter for doing fast searches are good but there is no identifier for the new ones. i think that there should be some kind of warning, either next to the icon that identifies each application, more or less for the time of a week, so reconcoer which are the new applications. \n",
      "\n",
      "Original Title:\n",
      " how to know which apps are recently released\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " new address for the new business page\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 39395 =================\n",
      "\n",
      "\"https://github.com/jeremyruppel/walrus/issues/30\"\n",
      "Issue Body:\n",
      " when i was trying to compile the template without the friends array because data is dynamic : html <h1>{{name.first}} {{name.last}}</h1> <ul> {{:each @friends do}} <li>{{name}}</li> {{end}} </ul> i get this message: typeerror: cannot set property '$index' of undefined at walrus.js:1125:24 there is any way to catch the errors to send over the rest server to show an error into the frontend? i'm working over nodejs 7.x. btw the other stuff works really great! \n",
      "\n",
      "Original Title:\n",
      " catch errors event\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " cannot use without a non existent template\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 83146 =================\n",
      "\n",
      "\"https://github.com/analogdevicesinc/scopy/issues/62\"\n",
      "Issue Body:\n",
      " connect m2k. start scopy, select the detected device. press connect button. after the connection is made, select voltmeter feature. actual results: history time axis -> vertical; history voltage axis -> horizontal. expected results: reverse axis option should be introduced so that the user can switch axis: history time axis -> horizontal; history voltage axis -> vertical. \n",
      "\n",
      "Original Title:\n",
      " history reverse axis option\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " connect to the device\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 4605 =================\n",
      "\n",
      "\"https://github.com/part-up/part-up/issues/1427\"\n",
      "Issue Body:\n",
      " when you insert a lane on ff and ie, there is quite a long delay. in chrome the delay is smaller, but still noticeable. \n",
      "\n",
      "Original Title:\n",
      " lane insert delay\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " delay on chrome\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 83665 =================\n",
      "\n",
      "\"https://github.com/juju/charm-helpers/issues/27\"\n",
      "Issue Body:\n",
      " the docs on https://pythonhosted.org/charmhelpers/ seem to be pretty out of date with the title displaying v0.12.0. \n",
      "\n",
      "Original Title:\n",
      " docs on pythonhosted.org are out of date, stuck on v0.12.0\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " documentation is out of date\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 56266 =================\n",
      "\n",
      "\"https://github.com/Microsoft/LightGBM/issues/515\"\n",
      "Issue Body:\n",
      " i trained a lightgbm model, and i want to see the tree graph so as to know how the gbm classify the data. does lightgbm now support visualization? \n",
      "\n",
      "Original Title:\n",
      " does lightgbm now support visualization?\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " how to add a tree to the visualization\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 81531 =================\n",
      "\n",
      "\"https://github.com/kubernetes/kubernetes.github.io/issues/2491\"\n",
      "Issue Body:\n",
      " https://kubernetes.io/docs/tools/ page does not mention minikube . by design or an oversight? \n",
      "\n",
      "Original Title:\n",
      " minikube not listed in /docs/tools/\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " documentation is not working with this project\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 81271 =================\n",
      "\n",
      "\"https://github.com/jamestonkin/nss-backend-capstone-client/issues/5\"\n",
      "Issue Body:\n",
      " when the user clicks on the search input field in the navigation bar and the user types any word when pressed the enter key then a hyperlink to any bill or resolution shows on the page \n",
      "\n",
      "Original Title:\n",
      " user can search for any bill or resolution\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " user input fields in search bar\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 90011 =================\n",
      "\n",
      "\"https://github.com/rwtema/extrautilities/issues/1826\"\n",
      "Issue Body:\n",
      " extrautils2-1.12-1.7.2 forge: 2589 minecraft: 1.12.2 i have a biome marker for hell in my quantum quarry, but it digs up stone, grass and ore. and it does not give any nether resources at all. it is a custom pack. ! image https://user-images.githubusercontent.com/7138125/34439180-55f1e8ac-ecac-11e7-8326-5ac5405a16c1.png ! image https://user-images.githubusercontent.com/7138125/34439184-678c6fd8-ecac-11e7-802e-3e4ee894f698.png ! image https://user-images.githubusercontent.com/7138125/34439192-757f6f28-ecac-11e7-96d5-0c467f8b9642.png ! image https://user-images.githubusercontent.com/7138125/34439213-83de107e-ecac-11e7-8233-c07eae109941.png \n",
      "\n",
      "Original Title:\n",
      " bug quantum quarry not paying attention to biome marker\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " number 2 number 2 number 2 number 2 number 2 number 2\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1365 =================\n",
      "\n",
      "\"https://github.com/vuetifyjs/docs/issues/356\"\n",
      "Issue Body:\n",
      " several component examples use the class 'headline' that is defined in the typography stylus file. could we add documentation for this to the style > typography docs section? stylus file: https://github.com/vuetifyjs/vuetify/blob/1d6453009758f587d6b3a22bdab38e4936502d65/src/stylus/elements/_typography.styl suggested docs location: https://vuetifyjs.com/style/typography \n",
      "\n",
      "Original Title:\n",
      " missing docs about 'headline' class\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " documentation for class component\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 19995 =================\n",
      "\n",
      "\"https://github.com/tensorflow/tensorflow/issues/12225\"\n",
      "Issue Body:\n",
      " please go to stack overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow if you open a github issue, here is our policy: 1. it must be a bug or a feature request. 2. the form below must be filled out. 3. it shouldn't be a tensorboard issue. those go here https://github.com/tensorflow/tensorboard/issues . here's why we have that policy : tensorflow developers respond to issues. we want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. support only helps individuals. github also notifies thousands of people when issues are filed. we want them to see you communicating an interesting problem, rather than being redirected to stack overflow. ------------------------ system information not applicable, this is a general feature request. describe the problem describe the problem clearly here. be sure to convey here why it's a bug in tensorflow or a feature request. feature request: make checkpoint files portable. checkpoint files are not portable, because they use absolute paths. if i move the directory containing a trained graph, and then try to restore from a checkpoint, i get unable to match files errors because tensorflow does not know to look in the checkpoint directory for checkpoint files. \n",
      "\n",
      "Original Title:\n",
      " request: make tensorflow checkpoints portable\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " how to set the default value for the same time\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 92751 =================\n",
      "\n",
      "\"https://github.com/archivers-space/recipes/issues/1\"\n",
      "Issue Body:\n",
      " a python abstract base class a la java interface will serve as the basis for recipes, as well as a reference for implementations in other languages. user recipes should be classes that extend the abstract base class. recipes should fork this repo and implement their class. which properties and methods should be included the base class is open for discussion. discussion can happen in this thread, but specific requests and proposals should be opened as separate issues or pull requests. \n",
      "\n",
      "Original Title:\n",
      " implementation of abstract base class for recipes\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " abstract class base classes\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 5253 =================\n",
      "\n",
      "\"https://github.com/eclipse/kapua/issues/174\"\n",
      "Issue Body:\n",
      " spring 4.3.2 is affected by cve-2016-9878 see https://pivotal.io/security/cve-2016-9878 . \n",
      "\n",
      "Original Title:\n",
      " consider updating to spring 4.3.5\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " spring number 2 number 2 number 2 number 2 number 2 number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 26935 =================\n",
      "\n",
      "\"https://github.com/NYC-GIS-Group/datacatalog-react/issues/14\"\n",
      "Issue Body:\n",
      " if dataset is not in readytoload gdb, provide a function to first search records in the catalog query master table - if data is found, query for the most recent transaction and make that row the current - if data is not found, query sde for dataset name -- if found create a new transaction for that dataset \n",
      "\n",
      "Original Title:\n",
      " add universal 'find' function\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " dataset with no dataset\n"
     ]
    }
   ],
   "source": [
    "# this method displays the predictions on random rows of the holdout set\n",
    "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Adding embeddings\n",
      "100%|██████████| 900000/900000 [00:27<00:00, 32786.69it/s]\n",
      "WARNING:root:Building trees for similarity lookup.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "all_data_df = pd.read_csv(GITHUB_ISSUES)\n",
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf_rec = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                    decoder_preprocessor=title_pp,\n",
    "                                    seq2seq_model=seq2seq_Model)\n",
    "recsys_annoyobj = seq2seq_inf_rec.prepare_recommender(train_body_vecs, all_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
